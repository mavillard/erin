{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transcripts_df = pd.read_csv(\n",
    "    'data/out/transcripts_1.csv',\n",
    "    converters={'INTERVIEWERS': eval, 'INTERVIEWEES': eval, 'ALIASES': eval, 'INTERVIEW': eval},\n",
    ")\n",
    "transcripts_df = transcripts_df[['ID', 'INTERVIEWERS', 'INTERVIEWEES', 'ALIASES', 'INTERVIEW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>INTERVIEWERS</th>\n",
       "      <th>INTERVIEWEES</th>\n",
       "      <th>ALIASES</th>\n",
       "      <th>INTERVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aimee Johnson – 17 September 2010</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Aimee Johnson]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok.  We’re recording now, I’m sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anita Smith -</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Anita Smith]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, How did we use to use the environme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apollo Blackeagle – 27 October 2010</td>\n",
       "      <td>[Rick Fehr, David White]</td>\n",
       "      <td>[Apollo Blackeagle]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok, its October 27th I believe, we’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill Sands</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Bill Sands]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, In the past, there’s concern today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brenda Wheat – 24 May 2011</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Brenda Wheat]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, So what we’ll be using is just a li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID              INTERVIEWERS  \\\n",
       "0    Aimee Johnson – 17 September 2010               [Rick Fehr]   \n",
       "1                       Anita Smith -               [Dave White]   \n",
       "2  Apollo Blackeagle – 27 October 2010  [Rick Fehr, David White]   \n",
       "3                           Bill Sands              [Dave White]   \n",
       "4           Brenda Wheat – 24 May 2011               [Rick Fehr]   \n",
       "\n",
       "          INTERVIEWEES ALIASES  \\\n",
       "0      [Aimee Johnson]      []   \n",
       "1        [Anita Smith]      []   \n",
       "2  [Apollo Blackeagle]      []   \n",
       "3         [Bill Sands]      []   \n",
       "4       [Brenda Wheat]      []   \n",
       "\n",
       "                                           INTERVIEW  \n",
       "0  [(0, Rick, Ok.  We’re recording now, I’m sitti...  \n",
       "1  [(0, Dave, How did we use to use the environme...  \n",
       "2  [(0, Rick, Ok, its October 27th I believe, we’...  \n",
       "3  [(0, Dave, In the past, there’s concern today ...  \n",
       "4  [(0, Rick, So what we’ll be using is just a li...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>INTERVIEWERS</th>\n",
       "      <th>INTERVIEWEES</th>\n",
       "      <th>ALIASES</th>\n",
       "      <th>INTERVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aimee Johnson – 17 September 2010</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Aimee Johnson]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok.  We’re recording now, I’m sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anita Smith -</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Anita Smith]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, How did we use to use the environme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apollo Blackeagle – 27 October 2010</td>\n",
       "      <td>[Rick Fehr, David White]</td>\n",
       "      <td>[Apollo Blackeagle]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok, its October 27th I believe, we’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill Sands</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Bill Sands]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, In the past, there’s concern today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brenda Wheat – 24 May 2011</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Brenda Wheat]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, So what we’ll be using is just a li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID              INTERVIEWERS  \\\n",
       "0    Aimee Johnson – 17 September 2010               [Rick Fehr]   \n",
       "1                       Anita Smith -               [Dave White]   \n",
       "2  Apollo Blackeagle – 27 October 2010  [Rick Fehr, David White]   \n",
       "3                           Bill Sands              [Dave White]   \n",
       "4           Brenda Wheat – 24 May 2011               [Rick Fehr]   \n",
       "\n",
       "          INTERVIEWEES ALIASES  \\\n",
       "0      [Aimee Johnson]      []   \n",
       "1        [Anita Smith]      []   \n",
       "2  [Apollo Blackeagle]      []   \n",
       "3         [Bill Sands]      []   \n",
       "4       [Brenda Wheat]      []   \n",
       "\n",
       "                                           INTERVIEW  \n",
       "0  [(0, Rick, Ok.  We’re recording now, I’m sitti...  \n",
       "1  [(0, Dave, How did we use to use the environme...  \n",
       "2  [(0, Rick, Ok, its October 27th I believe, we’...  \n",
       "3  [(0, Dave, In the past, there’s concern today ...  \n",
       "4  [(0, Rick, So what we’ll be using is just a li...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = '\\n'.join([text for interview in transcripts_df.INTERVIEW for (index, name, text) in interview])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [token for token in tokens if token not in stopwords.words('english') and len(token) > MIN_LENGTH]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 3.85 s, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cleaned_all_text = clean_text(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = cleaned_all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter_all = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yea', 2440),\n",
       " ('like', 2210),\n",
       " ('would', 1934),\n",
       " ('know', 1891),\n",
       " ('people', 1362),\n",
       " ('used', 1308),\n",
       " ('yeah', 1307),\n",
       " ('hmm', 1243),\n",
       " ('one', 1236),\n",
       " ('get', 1151),\n",
       " ('remember', 1135),\n",
       " ('time', 1088),\n",
       " ('think', 1026),\n",
       " ('well', 1013),\n",
       " ('back', 1012),\n",
       " ('things', 985),\n",
       " ('got', 953),\n",
       " ('way', 870),\n",
       " ('going', 868),\n",
       " ('lot', 857),\n",
       " ('right', 848),\n",
       " ('could', 735),\n",
       " ('use', 693),\n",
       " ('see', 672),\n",
       " ('something', 658),\n",
       " ('say', 623),\n",
       " ('good', 603),\n",
       " ('come', 600),\n",
       " ('around', 569),\n",
       " ('thing', 560),\n",
       " ('said', 512),\n",
       " ('stuff', 499),\n",
       " ('water', 496),\n",
       " ('went', 494),\n",
       " ('laughs', 492),\n",
       " ('take', 473),\n",
       " ('always', 472),\n",
       " ('really', 469),\n",
       " ('even', 466),\n",
       " ('community', 461),\n",
       " ('little', 459),\n",
       " ('fish', 449),\n",
       " ('kind', 439),\n",
       " ('big', 437),\n",
       " ('put', 437),\n",
       " ('make', 390),\n",
       " ('never', 383),\n",
       " ('want', 375),\n",
       " ('still', 361),\n",
       " ('maybe', 357),\n",
       " ('recall', 355),\n",
       " ('hunting', 353),\n",
       " ('anything', 345),\n",
       " ('old', 345),\n",
       " ('years', 343),\n",
       " ('bush', 332),\n",
       " ('dad', 329),\n",
       " ('much', 327),\n",
       " ('area', 317),\n",
       " ('ever', 316),\n",
       " ('island', 309),\n",
       " ('different', 307),\n",
       " ('today', 301),\n",
       " ('long', 292),\n",
       " ('everything', 285),\n",
       " ('day', 275),\n",
       " ('land', 271),\n",
       " ('work', 271),\n",
       " ('everybody', 270),\n",
       " ('might', 268),\n",
       " ('kids', 265),\n",
       " ('home', 265),\n",
       " ('away', 264),\n",
       " ('look', 263),\n",
       " ('came', 259),\n",
       " ('house', 258),\n",
       " ('year', 257),\n",
       " ('first', 253),\n",
       " ('made', 251),\n",
       " ('walpole', 249),\n",
       " ('also', 248),\n",
       " ('family', 245),\n",
       " ('else', 243),\n",
       " ('place', 243),\n",
       " ('road', 241),\n",
       " ('marsh', 235),\n",
       " ('many', 229),\n",
       " ('whatever', 229),\n",
       " ('food', 226),\n",
       " ('guys', 225),\n",
       " ('along', 222),\n",
       " ('need', 221),\n",
       " ('two', 220),\n",
       " ('told', 216),\n",
       " ('another', 214),\n",
       " ('guess', 213),\n",
       " ('stories', 211),\n",
       " ('somebody', 208),\n",
       " ('bring', 207),\n",
       " ('eat', 207)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_all.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.7 s, sys: 76 ms, total: 29.7 s\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tagged_all_text = pos_tag(word_tokenize(all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 s, sys: 3.87 s, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "adjectives = [word.lower() for word, pos in tagged_all_text if word not in stopwords.words('english') and len(word) > MIN_LENGTH and pos.startswith('J')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_adjs = Counter(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 593),\n",
       " ('little', 444),\n",
       " ('big', 428),\n",
       " ('old', 332),\n",
       " ('different', 307),\n",
       " ('much', 242),\n",
       " ('many', 229),\n",
       " ('whole', 191),\n",
       " ('right', 181),\n",
       " ('long', 176),\n",
       " ('first', 168),\n",
       " ('fish', 152),\n",
       " ('last', 139),\n",
       " ('able', 129),\n",
       " ('next', 126),\n",
       " ('certain', 125),\n",
       " ('young', 124),\n",
       " ('great', 120),\n",
       " ('indian', 118),\n",
       " ('particular', 111),\n",
       " ('wild', 101),\n",
       " ('sure', 97),\n",
       " ('real', 91),\n",
       " ('high', 90),\n",
       " ('environmental', 85),\n",
       " ('important', 83),\n",
       " ('sweet', 82),\n",
       " ('inaudible', 80),\n",
       " ('hard', 79),\n",
       " ('new', 76),\n",
       " ('nice', 76),\n",
       " ('black', 73),\n",
       " ('white', 71),\n",
       " ('small', 71),\n",
       " ('traditional', 68),\n",
       " ('younger', 66),\n",
       " ('bad', 66),\n",
       " ('uh…', 63),\n",
       " ('open', 57),\n",
       " ('customary', 56),\n",
       " ('natural', 56),\n",
       " ('commercial', 55),\n",
       " ('older', 55),\n",
       " ('main', 54),\n",
       " ('full', 53),\n",
       " ('yea', 50),\n",
       " ('best', 50),\n",
       " ('common', 49),\n",
       " ('better', 49),\n",
       " ('past', 46),\n",
       " ('native', 45),\n",
       " ('and…', 44),\n",
       " ('invasive', 44),\n",
       " ('red', 43),\n",
       " ('public', 42),\n",
       " ('social', 41),\n",
       " ('early', 40),\n",
       " ('fresh', 40),\n",
       " ('dry', 39),\n",
       " ('rid', 38),\n",
       " ('it…', 38),\n",
       " ('specific', 38),\n",
       " ('late', 38),\n",
       " ('wrong', 37),\n",
       " ('huge', 36),\n",
       " ('outside', 36),\n",
       " ('enough', 36),\n",
       " ('like', 35),\n",
       " ('large', 35),\n",
       " ('least', 34),\n",
       " ('aware', 34),\n",
       " ('there…', 34),\n",
       " ('sustainable', 33),\n",
       " ('major', 33),\n",
       " ('healthy', 33),\n",
       " ('the…', 32),\n",
       " ('clean', 31),\n",
       " ('biggest', 31),\n",
       " ('hot', 31),\n",
       " ('ready', 29),\n",
       " ('live', 28),\n",
       " ('general', 28),\n",
       " ('geese', 28),\n",
       " ('alive', 28),\n",
       " ('residential', 27),\n",
       " ('available', 27),\n",
       " ('low', 27),\n",
       " ('less', 27),\n",
       " ('green', 26),\n",
       " ('non-native', 26),\n",
       " ('similar', 25),\n",
       " ('easy', 25),\n",
       " ('that…', 25),\n",
       " ('smaller', 25),\n",
       " ('human', 25),\n",
       " ('local', 24),\n",
       " ('deep', 24),\n",
       " ('plentiful', 24),\n",
       " ('extra', 23),\n",
       " ('north', 23)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_adjs.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.4 s, sys: 3.97 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nouns = [word.lower() for word, pos in tagged_all_text if word not in stopwords.words('english') and len(word) > MIN_LENGTH and pos.startswith('N')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_nouns = Counter(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yea', 1734),\n",
       " ('people', 1358),\n",
       " ('hmm', 1231),\n",
       " ('time', 1088),\n",
       " ('things', 985),\n",
       " ('way', 870),\n",
       " ('lot', 857),\n",
       " ('something', 642),\n",
       " ('thing', 560),\n",
       " ('water', 496),\n",
       " ('community', 461),\n",
       " ('stuff', 458),\n",
       " ('kind', 439),\n",
       " ('well', 346),\n",
       " ('years', 343),\n",
       " ('anything', 335),\n",
       " ('bush', 320),\n",
       " ('area', 317),\n",
       " ('island', 309),\n",
       " ('dad', 305),\n",
       " ('today', 299),\n",
       " ('everything', 280),\n",
       " ('day', 275),\n",
       " ('kids', 259),\n",
       " ('everybody', 259),\n",
       " ('house', 258),\n",
       " ('year', 257),\n",
       " ('land', 256),\n",
       " ('walpole', 242),\n",
       " ('home', 242),\n",
       " ('family', 242),\n",
       " ('road', 241),\n",
       " ('place', 239),\n",
       " ('did', 236),\n",
       " ('fish', 228),\n",
       " ('food', 226),\n",
       " ('right', 210),\n",
       " ('stories', 209),\n",
       " ('part', 202),\n",
       " ('marsh', 202),\n",
       " ('environment', 201),\n",
       " ('days', 201),\n",
       " ('somebody', 201),\n",
       " ('yeah', 200),\n",
       " ('name', 198),\n",
       " ('money', 195),\n",
       " ('laughs', 187),\n",
       " ('guys', 187),\n",
       " ('person', 184),\n",
       " ('river', 180),\n",
       " ('ducks', 179),\n",
       " ('wood', 172),\n",
       " ('use', 166),\n",
       " ('school', 166),\n",
       " ('hunting', 161),\n",
       " ('times', 160),\n",
       " ('areas', 159),\n",
       " ('hall', 158),\n",
       " ('deer', 151),\n",
       " ('trees', 149),\n",
       " ('fishing', 149),\n",
       " ('guy', 146),\n",
       " ('life', 146),\n",
       " ('side', 142),\n",
       " ('ways', 138),\n",
       " ('ones', 137),\n",
       " ('cause', 136),\n",
       " ('end', 130),\n",
       " ('language', 130),\n",
       " ('horses', 128),\n",
       " ('st.', 122),\n",
       " ('yea…', 120),\n",
       " ('ice', 120),\n",
       " ('work', 119),\n",
       " ('point', 116),\n",
       " ('duck', 115),\n",
       " ('muskrat', 115),\n",
       " ('nobody', 114),\n",
       " ('corn', 114),\n",
       " ('kinds', 113),\n",
       " ('bit', 112),\n",
       " ('winter', 112),\n",
       " ('huh', 110),\n",
       " ('ground', 110),\n",
       " ('species', 109),\n",
       " ('garden', 109),\n",
       " ('one', 109),\n",
       " ('law', 108),\n",
       " ('care', 108),\n",
       " ('rules', 108),\n",
       " ('nothing', 107),\n",
       " ('yep', 107),\n",
       " ('mom', 106),\n",
       " ('grass', 104),\n",
       " ('someone', 104),\n",
       " ('muskrats', 103),\n",
       " ('lake', 102),\n",
       " ('potawatomi', 100),\n",
       " ('concerns', 99),\n",
       " ('resources', 98)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_nouns.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 s, sys: 3.94 s, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "verbs = [word.lower() for word, pos in tagged_all_text if word not in stopwords.words('english') and len(word) > MIN_LENGTH and pos.startswith('V')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_verbs = Counter(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('know', 1867),\n",
       " ('used', 1307),\n",
       " ('remember', 1089),\n",
       " ('get', 1075),\n",
       " ('think', 992),\n",
       " ('got', 942),\n",
       " ('going', 868),\n",
       " ('see', 655),\n",
       " ('say', 616),\n",
       " ('come', 552),\n",
       " ('use', 516),\n",
       " ('said', 512),\n",
       " ('went', 492),\n",
       " ('take', 468),\n",
       " ('put', 426),\n",
       " ('make', 382),\n",
       " ('want', 371),\n",
       " ('recall', 327),\n",
       " ('laughs', 289),\n",
       " ('came', 259),\n",
       " ('made', 251),\n",
       " ('told', 212),\n",
       " ('coming', 206),\n",
       " ('look', 205),\n",
       " ('done', 201),\n",
       " ('guess', 197),\n",
       " ('getting', 197),\n",
       " ('bring', 195),\n",
       " ('need', 192),\n",
       " ('took', 190),\n",
       " ('hunting', 189),\n",
       " ('tell', 186),\n",
       " ('eat', 176),\n",
       " ('knew', 173),\n",
       " ('talking', 172),\n",
       " ('help', 169),\n",
       " ('give', 168),\n",
       " ('live', 163),\n",
       " ('call', 154),\n",
       " ('work', 152),\n",
       " ('keep', 147),\n",
       " ('says', 143),\n",
       " ('lived', 143),\n",
       " ('talk', 140),\n",
       " ('wanted', 140),\n",
       " ('thought', 138),\n",
       " ('start', 137),\n",
       " ('seen', 136),\n",
       " ('find', 129),\n",
       " ('mean', 128),\n",
       " ('goes', 127),\n",
       " ('looking', 126),\n",
       " ('gone', 126),\n",
       " ('called', 125),\n",
       " ('hunt', 125),\n",
       " ('hear', 123),\n",
       " ('happened', 122),\n",
       " ('cut', 118),\n",
       " ('saying', 114),\n",
       " ('taking', 114),\n",
       " ('comes', 112),\n",
       " ('trying', 111),\n",
       " ('yea', 111),\n",
       " ('started', 107),\n",
       " ('working', 106),\n",
       " ('heard', 99),\n",
       " ('believe', 98),\n",
       " ('let', 97),\n",
       " ('worked', 96),\n",
       " ('pick', 95),\n",
       " ('walk', 95),\n",
       " ('buy', 90),\n",
       " ('leave', 88),\n",
       " ('left', 87),\n",
       " ('set', 85),\n",
       " ('taught', 84),\n",
       " ('like', 83),\n",
       " ('using', 82),\n",
       " ('sell', 81),\n",
       " ('changed', 81),\n",
       " ('needed', 80),\n",
       " ('try', 80),\n",
       " ('mentioned', 78),\n",
       " ('asked', 78),\n",
       " ('brought', 76),\n",
       " ('growing', 74),\n",
       " ('saw', 73),\n",
       " ('looked', 71),\n",
       " ('making', 71),\n",
       " ('play', 70),\n",
       " ('fish', 69),\n",
       " ('gave', 68),\n",
       " ('stay', 67),\n",
       " ('talked', 65),\n",
       " ('ask', 65),\n",
       " ('seeing', 65),\n",
       " ('telling', 64),\n",
       " ('sit', 63),\n",
       " ('run', 62),\n",
       " ('seems', 61)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_verbs.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_names(names):\n",
    "    return [x.split()[0] for x in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_names(names):\n",
    "    r = []\n",
    "    for x in names:\n",
    "        if '(' in x:\n",
    "            x = x.split('(')[0].strip()\n",
    "        xs = x.split()\n",
    "        if len(xs) > 1:\n",
    "            ys = xs[1:]\n",
    "            r.extend(ys)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interviewees = list(set(y for x in transcripts_df.INTERVIEWEES for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interviewees_first_names = first_names(interviewees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interviewees_last_names = last_names(interviewees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aliases = list(set(z for x in transcripts_df.ALIASES for y in x for z in y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_interviewees_names = set(interviewees + interviewees_first_names + interviewees_last_names + aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aimee',\n",
       " 'Aimee Johnson',\n",
       " 'Anita',\n",
       " 'Anita Smith',\n",
       " 'Apollo',\n",
       " 'Apollo Blackeagle',\n",
       " 'Aquash',\n",
       " 'Archie',\n",
       " 'Baxter',\n",
       " 'Becky',\n",
       " 'Bill',\n",
       " 'Bill Sands',\n",
       " 'Blackbird',\n",
       " 'Blackeagle',\n",
       " 'Brenda',\n",
       " 'Brenda Wheat',\n",
       " 'Cal',\n",
       " 'Cameron',\n",
       " 'Carl',\n",
       " 'Carl Smith (Resource Protection Officer)',\n",
       " 'Carmen',\n",
       " 'Carmen Wrightman',\n",
       " 'Cheryl',\n",
       " 'Chief',\n",
       " 'Chief Gilbert',\n",
       " 'Chief Joseph Gilbert',\n",
       " 'Chris',\n",
       " 'Chris Riley',\n",
       " 'Daniel',\n",
       " 'Day',\n",
       " 'Dean',\n",
       " 'Dean Jacobs',\n",
       " 'Dot',\n",
       " 'Dot Peters',\n",
       " 'Elaine',\n",
       " 'Elaine Jacobs',\n",
       " 'Eli',\n",
       " 'Eli Baxter',\n",
       " 'Eliza',\n",
       " 'Eliza John',\n",
       " 'Eric',\n",
       " 'Eric Isaac',\n",
       " 'Gilbert',\n",
       " 'Greg',\n",
       " 'Greg Isaac',\n",
       " 'Gus',\n",
       " 'Harold',\n",
       " 'Harold Peters',\n",
       " 'Hoeksma',\n",
       " 'Isaac',\n",
       " 'Isabelle',\n",
       " 'Jacobs',\n",
       " 'Jasper',\n",
       " 'Jasper John',\n",
       " 'Jean',\n",
       " 'Jean Wrightman',\n",
       " 'Jen',\n",
       " 'Jennie',\n",
       " 'Jennie Blackbird',\n",
       " 'Jessica',\n",
       " 'Joanne',\n",
       " 'Joanne Day',\n",
       " 'John',\n",
       " 'Johnson',\n",
       " 'Jones',\n",
       " 'Joseph',\n",
       " 'Karen',\n",
       " 'Karen Lalleen',\n",
       " 'Kenneth',\n",
       " 'Kennon',\n",
       " 'Kennon Johnson',\n",
       " 'Kevin',\n",
       " 'Kevin Smith',\n",
       " 'Lalleen',\n",
       " 'Lee',\n",
       " 'Lee White',\n",
       " 'Linda',\n",
       " 'Linda White',\n",
       " 'Liz',\n",
       " 'Lizzie',\n",
       " 'Lizzie Isaac',\n",
       " 'Lloyd',\n",
       " 'Lloyd Day',\n",
       " 'Lyndsay',\n",
       " 'Lyndsay Sword',\n",
       " 'Mel',\n",
       " 'Mel Hoeksma',\n",
       " 'Mickey',\n",
       " 'Mickey Aquash',\n",
       " 'Myrna',\n",
       " 'Naomi',\n",
       " 'Naomi Williams',\n",
       " 'PD',\n",
       " 'Pat',\n",
       " 'Pat Riley',\n",
       " 'Patricia',\n",
       " 'Patty',\n",
       " 'Patty Isaac',\n",
       " 'Peters',\n",
       " 'Puppydog',\n",
       " 'Rachel',\n",
       " 'Ralph',\n",
       " 'Ralph Johnson',\n",
       " 'Ralph Jones',\n",
       " 'Riley',\n",
       " 'Rita',\n",
       " 'Rita Sands',\n",
       " 'Rose',\n",
       " 'Sands',\n",
       " 'Shirley',\n",
       " 'Smith',\n",
       " 'Stanley',\n",
       " 'Stuart',\n",
       " 'Suzie',\n",
       " 'Suzie Isaac',\n",
       " 'Suzie Jones',\n",
       " 'Sword',\n",
       " 'Terry',\n",
       " 'Terry Sands',\n",
       " 'Vernon',\n",
       " 'Vernon Jones',\n",
       " 'Wheat',\n",
       " 'White',\n",
       " 'Williams',\n",
       " 'Wrightman']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_interviewees_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_interviewees_names_dict = {\n",
    "    'Aimee': 'Aimee Johnson',\n",
    "    'Aimee Johnson': 'Aimee Johnson',\n",
    "#     'Andrew': 'Andrew Peters',\n",
    "#     'Andrew Peters': 'Andrew Peters',\n",
    "    'Anita': 'Anita Smith',\n",
    "    'Anita Smith': 'Anita Smith',\n",
    "    'Apollo': 'Apollo Blackeagle',\n",
    "    'Apollo Blackeagle': 'Apollo Blackeagle',\n",
    "    'Aquash': 'Mickey Aquash',\n",
    "    'Archie': 'Archie',\n",
    "    'Baxter': 'Eli Baxter',\n",
    "    'Becky': 'Becky',\n",
    "    'Bill': 'Bill Sands',\n",
    "    'Bill Sands': 'Bill Sands',\n",
    "    'Blackbird': 'Jennie Blackbird',\n",
    "    'Blackeagle': 'Apollo Blackeagle',\n",
    "    'Brenda': 'Brenda Wheat',\n",
    "    'Brenda Wheat': 'Brenda Wheat',\n",
    "    'Cal': 'Cal',\n",
    "    'Cameron': 'Cameron',\n",
    "    'Carl': 'Carl Smith',\n",
    "    'Carl Smith (Resource Protection Officer)': 'Carl Smith',\n",
    "    'Carmen': 'Carmen Wrightman',\n",
    "    'Carmen Wrightman': 'Carmen Wrightman',\n",
    "#     'Carrie': 'Carrie Isaac',\n",
    "#     'Carrie Isaac': 'Carrie Isaac',\n",
    "#     'Charles': 'Charles Wright',\n",
    "#     'Charles Wright': 'Charles Wright',\n",
    "    'Cheryl': 'Cheryl',\n",
    "    'Chief': 'Chief Gilbert',\n",
    "    'Chief Gilbert': 'Chief Gilbert',\n",
    "    'Chief Joseph Gilbert': 'Chief Gilbert',\n",
    "    'Chris': 'Chris Riley',\n",
    "    'Chris Riley': 'Chris Riley',\n",
    "    'Daniel': 'Daniel',\n",
    "#     'Darren': 'Darren',\n",
    "    'Day': '??? Day',\n",
    "    'Dean': 'Dean Jacobs',\n",
    "    'Dean Jacobs': 'Dean Jacobs',\n",
    "    'Dot': 'Dot Peters',\n",
    "    'Dot Peters': 'Dot Peters',\n",
    "#     'Doug': 'Doug',\n",
    "#     'Doug (Resource Protection Officer)': 'Doug',\n",
    "    'Elaine': 'Elaine Jacobs',\n",
    "    'Elaine Jacobs': 'Elaine Jacobs',\n",
    "    'Eli': 'Eli Baxter',\n",
    "    'Eli Baxter': 'Eli Baxter',\n",
    "    'Eliza': 'Eliza John',\n",
    "    'Eliza John': 'Eliza John',\n",
    "    'Eric': 'Eric Isaac',\n",
    "    'Eric Isaac': 'Eric Isaac',\n",
    "#     'Frank': 'Frank',\n",
    "#     'Georgina': 'Georgina',\n",
    "    'Gilbert': 'Chief Gilbert',\n",
    "    'Greg': 'Greg Isaac',\n",
    "    'Greg Isaac': 'Greg Isaac',\n",
    "    'Gus': 'Gus',\n",
    "    'Harold': 'Harold Peters',\n",
    "    'Harold Peters': 'Harold Peters',\n",
    "    'Hoeksma': 'Mel Hoeksma',\n",
    "    'Isaac': '??? Isaac',\n",
    "    'Isabelle': 'Isabelle',\n",
    "    'Jacobs': '??? Jacobs',\n",
    "#     'Jane': 'Jane Jacobs',\n",
    "#     'Jane Jacobs': 'Jane Jacobs',\n",
    "    'Jasper': 'Jasper John',\n",
    "    'Jasper John': 'Jasper John',\n",
    "    'Jean': 'Jean Wrightman',\n",
    "    'Jean Wrightman': 'Jean Wrightman',\n",
    "    'Jen': 'Jennie Blackbird',\n",
    "    'Jennie': 'Jennie Blackbird',\n",
    "    'Jennie Blackbird': 'Jennie Blackbird',\n",
    "#     'Jerome': 'Jerome',\n",
    "#     'Jerry': 'Jerry',\n",
    "    'Jessica': 'Jessica',\n",
    "    'Joanne': 'Joanne Day',\n",
    "    'Joanne Day': 'Joanne Day',\n",
    "#     'Joe': 'Joe Isaac',\n",
    "#     'Joe Isaac': 'Joe Isaac',\n",
    "    'John': 'John',\n",
    "    'Johnson': '??? Johnson',\n",
    "    'Jones': '??? Jones',\n",
    "    'Joseph': 'Chief Gilbert',\n",
    "#     'Julia': 'Julia',\n",
    "    'Karen': 'Karen Lalleen',\n",
    "    'Karen Lalleen': 'Karen Lalleen',\n",
    "    'Kenneth': 'Kenneth',\n",
    "    'Kennon': 'Kennon Johnson',\n",
    "    'Kennon Johnson': 'Kennon Johnson',\n",
    "    'Kevin': 'Kevin Smith',\n",
    "    'Kevin Smith': 'Kevin Smith',\n",
    "    'Lalleen': 'Karen Lalleen',\n",
    "    'Lee': 'Lee White',\n",
    "    'Lee White': 'Lee White',\n",
    "    'Linda': 'Linda White',\n",
    "    'Linda White': 'Linda White',\n",
    "    'Liz': 'Lizzie Isaac',\n",
    "    'Lizzie': 'Lizzie Isaac',\n",
    "    'Lizzie Isaac': 'Lizzie Isaac',\n",
    "    'Lloyd': 'Lloyd Day',\n",
    "    'Lloyd Day': 'Lloyd Day',\n",
    "    'Lois': 'Lois Wrightman',\n",
    "    'Lois Wrightman': 'Lois Wrightman',\n",
    "    'Lyndsay': 'Lyndsay Sword',\n",
    "    'Lyndsay Sword': 'Lyndsay Sword',\n",
    "#     'Mark': 'Mark',\n",
    "    'Mel': 'Mel Hoeksma',\n",
    "    'Mel Hoeksma': 'Mel Hoeksma',\n",
    "    'Mickey': 'Mickey Aquash',\n",
    "    'Mickey Aquash': 'Mickey Aquash',\n",
    "#     'Morris': 'Morris Wrightman',\n",
    "#     'Morris Wrightman': 'Morris Wrightman',\n",
    "    'Myrna': 'Myrna',\n",
    "    'Naomi': 'Naomi Williams',\n",
    "    'Naomi Williams': 'Naomi Williams',\n",
    "    'PD': 'Puppydog',\n",
    "    'Pat': 'Pat Riley',\n",
    "    'Pat Riley': 'Pat Riley',\n",
    "    'Patricia': 'Patricia',\n",
    "    'Patty': 'Patty Isaac',\n",
    "    'Patty Isaac': 'Patty Isaac',\n",
    "#     'Paul': 'Paul',\n",
    "    'Peters': '??? Peters',\n",
    "    'Puppydog': 'Puppydog',\n",
    "    'Rachel': 'Rachel',\n",
    "    'Ralph': 'Ralph ???',\n",
    "    'Ralph Johnson': 'Ralph Johnson',\n",
    "    'Ralph Jones': 'Ralph Jones',\n",
    "    'Riley': '??? Riley',\n",
    "    'Rita': 'Rita Sands',\n",
    "    'Rita Sands': 'Rita Sands',\n",
    "#     'Ron': 'Ron',\n",
    "    'Rose': 'Rose',\n",
    "    'Sands': '??? Sands',\n",
    "#     'Sarah': 'Sarah',\n",
    "    'Shirley': 'Shirley',\n",
    "    'Smith': '??? Smith',\n",
    "    'Stanley': 'Stanley',\n",
    "    'Stuart': 'Stuart',\n",
    "    'Suzie': 'Suzie ???',\n",
    "    'Suzie Isaac': 'Suzie Isaac',\n",
    "    'Suzie Jones': 'Suzie Jones',\n",
    "    'Sword': 'Lyndsay Sword',\n",
    "    'Terry': 'Terry Sands',\n",
    "    'Terry Sands': 'Terry Sands',\n",
    "#     'Tom': 'Tom',\n",
    "    'Vernon': 'Vernon Jones',\n",
    "    'Vernon Jones': 'Vernon Jones',\n",
    "    'Wheat': 'Brenda Wheat',\n",
    "    'White': '??? White',\n",
    "    'Williams': 'Naomi Williams',\n",
    "#     'Wright': 'Charles Wright',\n",
    "    'Wrightman': '??? Wrightman',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = 'PROPER_NOUN: {<NNP>+}'\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 72 ms, total: 22.6 s\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g = nx.DiGraph()\n",
    "interviewers_names = ['Dave', 'Rick', 'Clint']\n",
    "for interview in transcripts_df.INTERVIEW:\n",
    "    for index, name, text in interview:\n",
    "        if text:\n",
    "            if name not in interviewers_names:\n",
    "                name = all_interviewees_names_dict[name]\n",
    "                if not g.has_node(name):\n",
    "                    g.add_node(name)\n",
    "                tagged_text = pos_tag(word_tokenize(text))\n",
    "                parsed_text = cp.parse(tagged_text)\n",
    "                for e in parsed_text:\n",
    "                    if isinstance(e, nltk.tree.Tree) and e.label() == 'PROPER_NOUN':\n",
    "                        names = [word for word, tag in e if len(word) > 1]\n",
    "                        if all_interviewees_names.intersection(names):\n",
    "                            proper_noun = ' '.join(names)\n",
    "                            if proper_noun in all_interviewees_names_dict and \\\n",
    "                               '???' not in all_interviewees_names_dict[proper_noun]:\n",
    "                                proper_noun = all_interviewees_names_dict[proper_noun]\n",
    "                            if proper_noun != name:\n",
    "                                if not g.has_edge(name, proper_noun):\n",
    "                                    g.add_edge(name, proper_noun, weight=0)\n",
    "                                g[name][proper_noun]['weight'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continents_list = ['Europe', 'America', 'Asia', 'Africa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/places/countries.txt') as f:\n",
    "    countries_list = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Canadian main places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/places/canadian_main_places.txt') as f:\n",
    "    ca_main_places_list = [p.strip() for p in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other Canadian places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/places/canadian_places.txt') as f:\n",
    "    ca_other_places_list = [p.strip() for p in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Canadian cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ca_cities_list = []\n",
    "\n",
    "with open('data/places/canadian_cities.txt') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            city = line.split('\\t')[0]\n",
    "            city = city.replace('(part)', '')\n",
    "            if '[' in city:\n",
    "                city = city.split('[')[0]\n",
    "            ca_cities_list.append(city.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Canadian towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ca_towns_list = []\n",
    "\n",
    "with open('data/places/canadian_towns.txt') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            town = line.split('\\t')[0]\n",
    "            town = town.replace('–', '')\n",
    "            if '[' in town:\n",
    "                town = town.split('[')[0]\n",
    "            if '(' in town:\n",
    "                town = town.split('(')[0]\n",
    "            ca_towns_list.append(town.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### US states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/places/us_states.txt') as f:\n",
    "    us_states_list = [p.strip() for p in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### US cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_cities_list = []\n",
    "\n",
    "with open('data/places/us_cities.txt') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            city = line.split('\\t')[1]\n",
    "            city = city.replace('–', '')\n",
    "            if '[' in city:\n",
    "                city = city.split('[')[0]\n",
    "            us_cities_list.append(city.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Michigan cities and towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi_cities_towns_list = []\n",
    "\n",
    "with open('data/places/mi_cities_towns.txt') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            city = line.split('\\t')[0]\n",
    "            if ',' in city:\n",
    "                city = city.split(',')[0]\n",
    "            mi_cities_towns_list.append(city.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New York cities and towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny_cities_towns_list = []\n",
    "\n",
    "with open('data/places/ny_cities_towns.txt') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            city = line.split('\\t')[0]\n",
    "            if '(' in city:\n",
    "                city = city.split('(')[0]\n",
    "            ny_cities_towns_list.append(city.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_places_list = \\\n",
    "    continents_list + countries_list + ca_main_places_list + ca_other_places_list + ca_cities_list + \\\n",
    "    ca_towns_list + us_states_list + us_cities_list + mi_cities_towns_list + ny_cities_towns_list\n",
    "all_places_lower_list = [p.lower() for p in all_places_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/wordsEn.txt') as f:\n",
    "    english_words = set(word.strip().lower() for word in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "changes_dict = {\n",
    "    'AHHHH': '',\n",
    "    'Alan Delery': 'Alan Deleary',\n",
    "    'All-righty': '',\n",
    "    'Altiman Rd': 'Altiman Road',\n",
    "    'Altiman Road': 'Altiman Road',\n",
    "    'Altman': 'Altiman',\n",
    "    'Amherstberg': 'Amherstburg',\n",
    "    'Anglican Hall': 'Parish Hall',\n",
    "    'Anglican Parish': 'Parish Hall',\n",
    "    'Anglican Parish Hall': 'Parish Hall',\n",
    "    'Anishibaabeg Maatisowin': 'Anishinaabe Maatisowin',\n",
    "    'Anishinaabeg': 'Anishinaabe',\n",
    "    'Anishinaabg': 'Anishinaabe',\n",
    "    'Anishinaabge': 'Anishinaabe',\n",
    "    'Anishna': 'Anishinaabe',\n",
    "    'Anishnaabe': 'Anishinaabe',\n",
    "    'Anishnabe': 'Anishinaabe',\n",
    "    'Attapiskatt First Nation': 'Attawapiskat First Nation',\n",
    "    'Attawapiskatt': 'Attawapiskat First Nation',\n",
    "    'Austin Rd': 'Austin Road',\n",
    "    'Basset Island': 'Bassett Island',\n",
    "    'Became menstrual': 'Bassett Island',\n",
    "    'Cecelia': 'Cecilia',\n",
    "    'Cecile': 'Cecilia',\n",
    "    'Cedrick': 'Cedric',\n",
    "    'Chematagon': 'Chematogan',\n",
    "    'Chematogon': 'Chematogan',\n",
    "    'Chematogon Bay': 'Chematogan Bay',\n",
    "    'Chester Armstrong Well': 'Chester Armstrong',\n",
    "    'Chimey': 'Chimmy',\n",
    "    'Clifford Roy': 'Cliff Roy',\n",
    "    'Clint Yeah': 'Clint',\n",
    "    'DUNK DUNK DUNK': 'DUNK DUNK',\n",
    "    'Dave Did': 'Dave',\n",
    "    'Dave Do': 'Dave',\n",
    "    'Dredgecut': 'Dredge Cut',\n",
    "    'Enh': '',\n",
    "    'Ethel Kick': 'Ethel Kicknosway',\n",
    "    'First Nation': 'First Nations',\n",
    "    'Frank Clown Dawson': 'Franklin Dawson',\n",
    "    'Franny': 'Frannie',\n",
    "    'Fred Hall': 'Fred Hulls',\n",
    "    'George uh': 'George',\n",
    "    'Gooselake': 'Goose Lake',\n",
    "    'Gzeh-mnidoo': 'Gzhe-mnidoo',\n",
    "    'He/we': '',\n",
    "    'Hello Aklyn': 'Aklyn',\n",
    "    'Heriatge Centre': 'Heritage Centre',\n",
    "    'Heritage Center': 'Heritage Centre',\n",
    "    'Hickory Ridges': 'Hickory Ridge',\n",
    "    'Highbanks yea': 'Highbanks',\n",
    "    'Hm': '',\n",
    "    'Hmm': '',\n",
    "    'Hmm-mm': '',\n",
    "    'I-I-I': 'I-I',\n",
    "    'If-if': '',\n",
    "    'Igotta': '',\n",
    "    'It-it': '',\n",
    "    'Jan Longboats': 'Jan Longboat',\n",
    "    'Jeffrey': 'Jeffery',\n",
    "    'Joe Bidore Bay': 'Joe Bedore Bay',\n",
    "    'Joe Crows': 'Joe Crow',\n",
    "    'Kewayosh-': 'Kewayosh',\n",
    "    'Kicknosways': 'Kicknosway',\n",
    "    'Laughs Nobody': '',\n",
    "    'Laughs So': '',\n",
    "    'Laughs Walpole Island': 'Walpole Island',\n",
    "    'Laughs Yea': '',\n",
    "    'Laughs Yea Robert Kiyoshk': 'Robert Kiyoshk',\n",
    "    'Mid-winter Pow-wow': 'Mid-winter Pow-Wow',\n",
    "    'Mitchells Bay': 'Mitchell Bay',\n",
    "    'Mm': '',\n",
    "    'Mm-hmm': '',\n",
    "    'Mrs': '',\n",
    "    'Munja': 'Moonja',\n",
    "    'Nahdees': 'Nahdee',\n",
    "    'Native Crafts.': 'Native Crafts',\n",
    "    'New Years': 'New Year',\n",
    "    'New Years Eve': 'New Year',\n",
    "    'New Years Feast': 'New Year',\n",
    "    'Nishnaabs': 'Nishnaabe',\n",
    "    'Nishnob': 'Nishnaabe',\n",
    "    'Nishnobs': 'Nishnaabe',\n",
    "    'Odawas': 'Odawa',\n",
    "    'Oh God': '',\n",
    "    'Ojibwe Park': 'Ojibway Park',\n",
    "    'Ok Dorothy': 'Dorothy',\n",
    "    'Ok Henry': 'Henry',\n",
    "    'POW WOW': 'Pow-Wow',\n",
    "    'POW WOWS': 'Pow-Wow',\n",
    "    'POW-Wow': 'Pow-Wow',\n",
    "    'Parrish Hall': 'Parish Hall',\n",
    "    'Potawatomis': 'Potawatomi',\n",
    "    'Pottawatomi': 'Potawatomi',\n",
    "    'Pottawatomi Island': 'Potawatomi Island',\n",
    "    'Pow-Wows': 'Pow-Wow',\n",
    "    'Pow-wow': 'Pow-Wow',\n",
    "    'Rama Powwow': 'Rama Pow-Wow',\n",
    "    'Rodeo': 'Rodeo',\n",
    "    'Rommel': 'Romall',\n",
    "    'Rommels': 'Romall',\n",
    "    'Rumall': 'Romall',\n",
    "    'Russel Osagee': 'Russell Osagee',\n",
    "    'SHHHHHHH': '',\n",
    "    'Sam Weegee': 'Sam Weejii',\n",
    "    'Sam Weegy': 'Sam Weejii',\n",
    "    'Sam Weeji': 'Sam Weejii',\n",
    "    'Sam Wiiji': 'Sam Weejii',\n",
    "    'Sam Wiji': 'Sam Weejii',\n",
    "    'Same Wiji': 'Sam Weejii',\n",
    "    'Sanjgwan': 'Sanjgwon',\n",
    "    'Shingauk': 'Shingwauk',\n",
    "    'Shinguak': 'Shingwauk',\n",
    "    'Shingwak': 'Shingwauk',\n",
    "    'Shingwuak': 'Shingwauk',\n",
    "    'Shobs': 'Shob',\n",
    "    'Shogie': 'Shoggie',\n",
    "    'Skeesic': 'Skeezik',\n",
    "    'Skiizhiig': 'Skeezik',\n",
    "    'So Potawatomi': 'Potawatomi',\n",
    "    'Soo': '',\n",
    "    'St. Ann': 'St. Anne',\n",
    "    'Strawberry Soc': 'Strawberry Social',\n",
    "    'T.V.': 'T.V',\n",
    "    'Thank-you': '',\n",
    "    'Trans Canada Pipeline': 'Transport Canada',\n",
    "    'Twila': 'Twyla',\n",
    "    'U.S.': 'U.S',\n",
    "    'Uh Aldra Brown': 'Aldra Brown',\n",
    "    'Uh Eldron': 'Eldron',\n",
    "    'Uh Henry': 'Henry',\n",
    "    'Um': '',\n",
    "    'Umm': '',\n",
    "    'Walpole Island.': 'Walpole Island',\n",
    "    'Weekaan': 'Wiikenh',\n",
    "    'Wood Bees': 'Wood Bee',\n",
    "    'Woodcutting Bee': 'Wood Bee',\n",
    "    'World War II': 'World War',\n",
    "    'Yea Bassett': 'Bassett',\n",
    "    'Yea Chiefs Road': 'Chiefs Road',\n",
    "    'Yea Quopkaing': 'Quopkaing',\n",
    "    'Yea Winston': 'Winston',\n",
    "    'Yea uh': '',\n",
    "    'Yep uh': '',\n",
    "    'Zhoon ya': 'Zhoon',\n",
    "    'knock knock': '',\n",
    "    'yea uh': '',\n",
    "    'yea yep': '',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_ellipsis(text):\n",
    "    text = text.replace('…', '...')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(names):\n",
    "    return names and \\\n",
    "           (' '.join(names).lower() in all_places_lower_list or \\\n",
    "           not all_interviewees_names.intersection(names) and \\\n",
    "           (len(names) > 1 or names[0].lower().strip(punctuation + ' ') not in english_words)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 48 ms, total: 23.2 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "h = nx.DiGraph()\n",
    "interviewers_names = ['Dave', 'Rick', 'Clint']\n",
    "for interview in transcripts_df.INTERVIEW:\n",
    "    for index, name, text in interview:\n",
    "        if text:\n",
    "            text = fix_ellipsis(text)\n",
    "            if name not in interviewers_names:\n",
    "                name = all_interviewees_names_dict[name]\n",
    "                if not h.has_node(name):\n",
    "                    h.add_node(name, type='interviewee')\n",
    "                tagged_text = pos_tag(word_tokenize(text))\n",
    "                parsed_text = cp.parse(tagged_text)\n",
    "                for e in parsed_text:\n",
    "                    if isinstance(e, nltk.tree.Tree) and e.label() == 'PROPER_NOUN':\n",
    "                        names = [word for word, tag in e if len(word) > 1]\n",
    "                        if test(names):\n",
    "                            proper_noun = ' '.join(names)\n",
    "                            proper_noun = changes_dict.get(proper_noun, proper_noun)\n",
    "                            if proper_noun != name:\n",
    "                                if not h.has_node(proper_noun):\n",
    "                                    h.add_node(proper_noun, type='other')\n",
    "                                if not h.has_edge(name, proper_noun):\n",
    "                                    h.add_edge(name, proper_noun, weight=0)\n",
    "                                h[name][proper_noun]['weight'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = h.copy()\n",
    "for n in i.nodes():\n",
    "    if len(n) <= MIN_LENGTH:\n",
    "        i.remove_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1545"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/all_text.txt', 'w') as f:\n",
    "    f.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/counter_all.pickle', 'wb') as f:\n",
    "    pickle.dump(counter_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/counter_adjs.pickle', 'wb') as f:\n",
    "    pickle.dump(counter_adjs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/counter_nouns.pickle', 'wb') as f:\n",
    "    pickle.dump(counter_nouns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/counter_verbs.pickle', 'wb') as f:\n",
    "    pickle.dump(counter_verbs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(g, 'data/out/people.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(i, 'data/out/other.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
