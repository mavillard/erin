{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transcripts_df = pd.read_csv(\n",
    "    'data/out/transcripts_1.csv',\n",
    "    converters={'INTERVIEWERS': eval, 'INTERVIEWEES': eval, 'ALIASES': eval, 'INTERVIEW': eval},\n",
    ")\n",
    "transcripts_df = transcripts_df[['ID', 'INTERVIEWERS', 'INTERVIEWEES', 'ALIASES', 'INTERVIEW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>INTERVIEWERS</th>\n",
       "      <th>INTERVIEWEES</th>\n",
       "      <th>ALIASES</th>\n",
       "      <th>INTERVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aimee Johnson – 17 September 2010</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Aimee Johnson]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok.  We’re recording now, I’m sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anita Smith -</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Anita Smith]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, How did we use to use the environme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apollo Blackeagle – 27 October 2010</td>\n",
       "      <td>[Rick Fehr, David White]</td>\n",
       "      <td>[Apollo Blackeagle]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok, its October 27th I believe, we’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill Sands</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Bill Sands]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, In the past, there’s concern today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brenda Wheat – 24 May 2011</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Brenda Wheat]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, So what we’ll be using is just a li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID              INTERVIEWERS  \\\n",
       "0    Aimee Johnson – 17 September 2010               [Rick Fehr]   \n",
       "1                       Anita Smith -               [Dave White]   \n",
       "2  Apollo Blackeagle – 27 October 2010  [Rick Fehr, David White]   \n",
       "3                           Bill Sands              [Dave White]   \n",
       "4           Brenda Wheat – 24 May 2011               [Rick Fehr]   \n",
       "\n",
       "          INTERVIEWEES ALIASES  \\\n",
       "0      [Aimee Johnson]      []   \n",
       "1        [Anita Smith]      []   \n",
       "2  [Apollo Blackeagle]      []   \n",
       "3         [Bill Sands]      []   \n",
       "4       [Brenda Wheat]      []   \n",
       "\n",
       "                                           INTERVIEW  \n",
       "0  [(0, Rick, Ok.  We’re recording now, I’m sitti...  \n",
       "1  [(0, Dave, How did we use to use the environme...  \n",
       "2  [(0, Rick, Ok, its October 27th I believe, we’...  \n",
       "3  [(0, Dave, In the past, there’s concern today ...  \n",
       "4  [(0, Rick, So what we’ll be using is just a li...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>INTERVIEWERS</th>\n",
       "      <th>INTERVIEWEES</th>\n",
       "      <th>ALIASES</th>\n",
       "      <th>INTERVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aimee Johnson – 17 September 2010</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Aimee Johnson]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok.  We’re recording now, I’m sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anita Smith -</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Anita Smith]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, How did we use to use the environme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apollo Blackeagle – 27 October 2010</td>\n",
       "      <td>[Rick Fehr, David White]</td>\n",
       "      <td>[Apollo Blackeagle]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, Ok, its October 27th I believe, we’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill Sands</td>\n",
       "      <td>[Dave White]</td>\n",
       "      <td>[Bill Sands]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Dave, In the past, there’s concern today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brenda Wheat – 24 May 2011</td>\n",
       "      <td>[Rick Fehr]</td>\n",
       "      <td>[Brenda Wheat]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, Rick, So what we’ll be using is just a li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID              INTERVIEWERS  \\\n",
       "0    Aimee Johnson – 17 September 2010               [Rick Fehr]   \n",
       "1                       Anita Smith -               [Dave White]   \n",
       "2  Apollo Blackeagle – 27 October 2010  [Rick Fehr, David White]   \n",
       "3                           Bill Sands              [Dave White]   \n",
       "4           Brenda Wheat – 24 May 2011               [Rick Fehr]   \n",
       "\n",
       "          INTERVIEWEES ALIASES  \\\n",
       "0      [Aimee Johnson]      []   \n",
       "1        [Anita Smith]      []   \n",
       "2  [Apollo Blackeagle]      []   \n",
       "3         [Bill Sands]      []   \n",
       "4       [Brenda Wheat]      []   \n",
       "\n",
       "                                           INTERVIEW  \n",
       "0  [(0, Rick, Ok.  We’re recording now, I’m sitti...  \n",
       "1  [(0, Dave, How did we use to use the environme...  \n",
       "2  [(0, Rick, Ok, its October 27th I believe, we’...  \n",
       "3  [(0, Dave, In the past, there’s concern today ...  \n",
       "4  [(0, Rick, So what we’ll be using is just a li...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = '\\n'.join([text for interview in transcripts_df.INTERVIEW for (index, name, text) in interview])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/all_text.txt', 'w') as f:\n",
    "    f.write(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [token for token in tokens if token not in stopwords.words('english') and len(token) > 2]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 5.13 s, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cleaned_all_text = clean_text(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = cleaned_all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter_all = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 2817),\n",
       " ('would', 2627),\n",
       " ('know', 2450),\n",
       " ('yea', 2440),\n",
       " ('people', 1995),\n",
       " ('yeah', 1924),\n",
       " ('go', 1883),\n",
       " ('used', 1824),\n",
       " ('one', 1702),\n",
       " ('remember', 1549),\n",
       " ('get', 1545),\n",
       " ('time', 1537),\n",
       " ('uh', 1517),\n",
       " ('think', 1465),\n",
       " ('things', 1404),\n",
       " ('well', 1390),\n",
       " ('back', 1364),\n",
       " ('got', 1273),\n",
       " ('going', 1250),\n",
       " ('hmm', 1245),\n",
       " ('way', 1204),\n",
       " ('lot', 1182),\n",
       " ('right', 1116),\n",
       " ('could', 990),\n",
       " ('something', 930),\n",
       " ('see', 905),\n",
       " ('say', 856),\n",
       " ('use', 839),\n",
       " ('come', 818),\n",
       " ('good', 812),\n",
       " ('thing', 787),\n",
       " ('around', 771),\n",
       " ('said', 766),\n",
       " ('us', 741),\n",
       " ('oh', 706),\n",
       " ('community', 689),\n",
       " ('water', 677),\n",
       " ('really', 673),\n",
       " ('went', 664),\n",
       " ('stuff', 656),\n",
       " ('always', 643),\n",
       " ('even', 639),\n",
       " ('take', 633),\n",
       " ('laughs', 631),\n",
       " ('fish', 624),\n",
       " ('little', 598),\n",
       " ('kind', 596),\n",
       " ('put', 585),\n",
       " ('big', 577),\n",
       " ('recall', 545),\n",
       " ('hunting', 535),\n",
       " ('um', 520),\n",
       " ('never', 519),\n",
       " ('want', 516),\n",
       " ('make', 509),\n",
       " ('still', 489),\n",
       " ('years', 483),\n",
       " ('today', 483),\n",
       " ('old', 477),\n",
       " ('dad', 475),\n",
       " ('maybe', 459),\n",
       " ('area', 451),\n",
       " ('bush', 450),\n",
       " ('much', 449),\n",
       " ('anything', 445),\n",
       " ('island', 425),\n",
       " ('eh', 414),\n",
       " ('everything', 413),\n",
       " ('ever', 411),\n",
       " ('day', 411),\n",
       " ('home', 405),\n",
       " ('long', 403),\n",
       " ('land', 394),\n",
       " ('everybody', 393),\n",
       " ('might', 386),\n",
       " ('different', 383),\n",
       " ('kids', 375),\n",
       " ('work', 368),\n",
       " ('look', 365),\n",
       " ('marsh', 360),\n",
       " ('house', 356),\n",
       " ('made', 354),\n",
       " ('walpole', 352),\n",
       " ('away', 351),\n",
       " ('year', 349),\n",
       " ('came', 343),\n",
       " ('also', 339),\n",
       " ('family', 338),\n",
       " ('road', 338),\n",
       " ('need', 337),\n",
       " ('else', 327),\n",
       " ('first', 327),\n",
       " ('guys', 323),\n",
       " ('place', 321),\n",
       " ('food', 321),\n",
       " ('guess', 319),\n",
       " ('whatever', 316),\n",
       " ('another', 311),\n",
       " ('many', 308),\n",
       " ('two', 308)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_all.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.1 s, sys: 80 ms, total: 38.1 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tagged_all_text = pos_tag(word_tokenize(all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 5 s, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "adjectives = [word.lower() for word, pos in tagged_all_text if word not in stopwords.words('english') and len(word) > 1 and pos.startswith('J')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_adjs = Counter(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 799),\n",
       " ('little', 579),\n",
       " ('big', 563),\n",
       " ('uh', 550),\n",
       " ('old', 462),\n",
       " ('different', 383),\n",
       " ('much', 324),\n",
       " ('many', 308),\n",
       " ('um', 273),\n",
       " ('whole', 273),\n",
       " ('long', 245),\n",
       " ('right', 228),\n",
       " ('first', 222),\n",
       " ('fish', 200),\n",
       " ('last', 199),\n",
       " ('certain', 181),\n",
       " ('able', 179),\n",
       " ('particular', 171),\n",
       " ('next', 168),\n",
       " ('young', 166),\n",
       " ('great', 166),\n",
       " ('indian', 153),\n",
       " ('wild', 146),\n",
       " ('high', 129),\n",
       " ('sure', 125),\n",
       " ('real', 119),\n",
       " ('important', 114),\n",
       " ('environmental', 114),\n",
       " ('hard', 112),\n",
       " ('black', 107),\n",
       " ('new', 104),\n",
       " ('nice', 102),\n",
       " ('white', 99),\n",
       " ('sweet', 96),\n",
       " ('inaudible', 92),\n",
       " ('bad', 91),\n",
       " ('small', 90),\n",
       " ('customary', 89),\n",
       " ('natural', 88),\n",
       " ('traditional', 87),\n",
       " ('younger', 85),\n",
       " ('main', 80),\n",
       " ('common', 75),\n",
       " ('best', 75),\n",
       " ('commercial', 73),\n",
       " ('open', 73),\n",
       " ('older', 69),\n",
       " ('native', 69),\n",
       " ('full', 66),\n",
       " ('past', 65),\n",
       " ('social', 64),\n",
       " ('uh…', 63),\n",
       " ('better', 63),\n",
       " ('red', 60),\n",
       " ('fresh', 59),\n",
       " ('rid', 58),\n",
       " ('public', 58),\n",
       " ('late', 58),\n",
       " ('large', 56),\n",
       " ('early', 55),\n",
       " ('invasive', 55),\n",
       " ('wrong', 55),\n",
       " ('specific', 53),\n",
       " ('ok', 53),\n",
       " ('least', 52),\n",
       " ('outside', 51),\n",
       " ('yea', 50),\n",
       " ('dry', 48),\n",
       " ('biggest', 48),\n",
       " ('enough', 48),\n",
       " ('live', 48),\n",
       " ('huge', 46),\n",
       " ('hot', 45),\n",
       " ('major', 45),\n",
       " ('healthy', 44),\n",
       " ('aware', 44),\n",
       " ('like', 44),\n",
       " ('non-native', 44),\n",
       " ('and…', 44),\n",
       " ('geese', 41),\n",
       " ('clean', 41),\n",
       " ('local', 41),\n",
       " ('residential', 40),\n",
       " ('available', 40),\n",
       " ('less', 40),\n",
       " ('sustainable', 39),\n",
       " ('human', 39),\n",
       " ('general', 39),\n",
       " ('ready', 38),\n",
       " ('it…', 38),\n",
       " ('alive', 38),\n",
       " ('similar', 37),\n",
       " ('easy', 36),\n",
       " ('plentiful', 36),\n",
       " ('green', 35),\n",
       " ('marsh', 34),\n",
       " ('smaller', 34),\n",
       " ('there…', 34),\n",
       " ('couple', 33),\n",
       " ('the…', 33)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_adjs.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 5.25 s, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nouns = [word.lower() for word, pos in tagged_all_text if word not in stopwords.words('english') and len(word) > 1 and pos.startswith('N')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_nouns = Counter(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 1990),\n",
       " ('yea', 1734),\n",
       " ('time', 1537),\n",
       " ('things', 1404),\n",
       " ('hmm', 1233),\n",
       " ('way', 1204),\n",
       " ('lot', 1182),\n",
       " ('something', 911),\n",
       " ('thing', 787),\n",
       " ('community', 689),\n",
       " ('water', 677),\n",
       " ('kind', 596),\n",
       " ('stuff', 596),\n",
       " ('uh', 496),\n",
       " ('years', 483),\n",
       " ('today', 481),\n",
       " ('well', 478),\n",
       " ('area', 451),\n",
       " ('dad', 434),\n",
       " ('bush', 433),\n",
       " ('anything', 432),\n",
       " ('island', 425),\n",
       " ('day', 411),\n",
       " ('everything', 405),\n",
       " ('everybody', 378),\n",
       " ('land', 372),\n",
       " ('kids', 368),\n",
       " ('home', 365),\n",
       " ('house', 356),\n",
       " ('year', 349),\n",
       " ('walpole', 341),\n",
       " ('road', 338),\n",
       " ('family', 335),\n",
       " ('fish', 327),\n",
       " ('food', 321),\n",
       " ('place', 317),\n",
       " ('marsh', 311),\n",
       " ('yeah', 307),\n",
       " ('stories', 303),\n",
       " ('did', 301),\n",
       " ('part', 288),\n",
       " ('money', 285),\n",
       " ('environment', 282),\n",
       " ('oh', 279),\n",
       " ('days', 277),\n",
       " ('person', 275),\n",
       " ('guys', 274),\n",
       " ('right', 272),\n",
       " ('laughs', 271),\n",
       " ('somebody', 268),\n",
       " ('name', 258),\n",
       " ('ducks', 255),\n",
       " ('school', 250),\n",
       " ('hunting', 246),\n",
       " ('river', 245),\n",
       " ('wood', 238),\n",
       " ('areas', 224),\n",
       " ('life', 223),\n",
       " ('hall', 219),\n",
       " ('times', 219),\n",
       " ('fishing', 212),\n",
       " ('deer', 209),\n",
       " ('trees', 204),\n",
       " ('use', 203),\n",
       " ('guy', 201),\n",
       " ('end', 188),\n",
       " ('ways', 185),\n",
       " ('ice', 183),\n",
       " ('side', 181),\n",
       " ('ones', 181),\n",
       " ('point', 179),\n",
       " ('horses', 175),\n",
       " ('muskrat', 170),\n",
       " ('winter', 167),\n",
       " ('corn', 162),\n",
       " ('eh', 162),\n",
       " ('language', 162),\n",
       " ('mom', 161),\n",
       " ('um', 157),\n",
       " ('work', 157),\n",
       " ('rules', 155),\n",
       " ('duck', 153),\n",
       " ('one', 152),\n",
       " ('st.', 152),\n",
       " ('respect', 151),\n",
       " ('muskrats', 150),\n",
       " ('care', 149),\n",
       " ('someone', 149),\n",
       " ('garden', 149),\n",
       " ('law', 148),\n",
       " ('ground', 148),\n",
       " ('nobody', 146),\n",
       " ('species', 143),\n",
       " ('cause', 143),\n",
       " ('lake', 142),\n",
       " ('kinds', 141),\n",
       " ('potawatomi', 140),\n",
       " ('ok', 139),\n",
       " ('nothing', 138),\n",
       " ('resources', 138)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_nouns.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 5.5 s, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "verbs = [word.lower() for word, pos in tagged_all_text if word not in stopwords.words('english') and len(word) > 1 and pos.startswith('V')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_verbs = Counter(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('know', 2420),\n",
       " ('go', 1859),\n",
       " ('used', 1823),\n",
       " ('remember', 1495),\n",
       " ('get', 1446),\n",
       " ('think', 1422),\n",
       " ('got', 1257),\n",
       " ('going', 1250),\n",
       " ('see', 881),\n",
       " ('say', 847),\n",
       " ('said', 766),\n",
       " ('come', 747),\n",
       " ('went', 662),\n",
       " ('take', 625),\n",
       " ('use', 621),\n",
       " ('put', 568),\n",
       " ('want', 510),\n",
       " ('recall', 502),\n",
       " ('make', 497),\n",
       " ('do', 428),\n",
       " ('made', 354),\n",
       " ('came', 343),\n",
       " ('laughs', 340),\n",
       " ('told', 299),\n",
       " ('need', 299),\n",
       " ('guess', 298),\n",
       " ('coming', 295),\n",
       " ('look', 287),\n",
       " ('hunting', 283),\n",
       " ('bring', 280),\n",
       " ('getting', 271),\n",
       " ('tell', 270),\n",
       " ('done', 256),\n",
       " ('took', 255),\n",
       " ('talking', 251),\n",
       " ('eat', 239),\n",
       " ('knew', 239),\n",
       " ('help', 236),\n",
       " ('give', 228),\n",
       " ('uh', 212),\n",
       " ('work', 211),\n",
       " ('says', 210),\n",
       " ('call', 208),\n",
       " ('live', 207),\n",
       " ('talk', 203),\n",
       " ('seen', 201),\n",
       " ('keep', 199),\n",
       " ('start', 197),\n",
       " ('lived', 191),\n",
       " ('wanted', 187),\n",
       " ('thought', 187),\n",
       " ('mean', 180),\n",
       " ('gone', 179),\n",
       " ('hunt', 177),\n",
       " ('called', 177),\n",
       " ('find', 177),\n",
       " ('happened', 175),\n",
       " ('hear', 171),\n",
       " ('saying', 171),\n",
       " ('looking', 169),\n",
       " ('goes', 167),\n",
       " ('started', 166),\n",
       " ('taking', 163),\n",
       " ('trying', 161),\n",
       " ('cut', 159),\n",
       " ('comes', 153),\n",
       " ('heard', 138),\n",
       " ('believe', 136),\n",
       " ('eh', 136),\n",
       " ('working', 135),\n",
       " ('is', 135),\n",
       " ('walk', 129),\n",
       " ('worked', 125),\n",
       " ('buy', 124),\n",
       " ('set', 124),\n",
       " ('pick', 124),\n",
       " ('brought', 124),\n",
       " ('let', 124),\n",
       " ('changed', 122),\n",
       " ('taught', 121),\n",
       " ('leave', 121),\n",
       " ('using', 121),\n",
       " ('asked', 120),\n",
       " ('left', 116),\n",
       " ('yea', 111),\n",
       " ('try', 111),\n",
       " ('telling', 110),\n",
       " ('like', 108),\n",
       " ('looked', 106),\n",
       " ('growing', 106),\n",
       " ('mentioned', 105),\n",
       " ('needed', 105),\n",
       " ('saw', 102),\n",
       " ('sit', 101),\n",
       " ('play', 100),\n",
       " ('run', 99),\n",
       " ('sell', 98),\n",
       " ('fish', 97),\n",
       " ('making', 97),\n",
       " ('gave', 95)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_verbs.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_names(names):\n",
    "    return [x.split()[0] for x in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_names(names):\n",
    "    r = []\n",
    "    for x in names:\n",
    "        if '(' in x:\n",
    "            x = x.split('(')[0].strip()\n",
    "        xs = x.split()\n",
    "        if len(xs) > 1:\n",
    "            ys = xs[1:]\n",
    "            r.extend(ys)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interviewees = list(set(y for x in transcripts_df.INTERVIEWEES for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interviewees_first_names = first_names(interviewees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interviewees_last_names = last_names(interviewees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aliases = list(set(z for x in transcripts_df.ALIASES for y in x for z in y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_names = set(interviewees + interviewees_first_names + interviewees_last_names + aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_names_dict = {\n",
    "    'Aimee': 'Aimee Johnson',\n",
    "    'Aimee Johnson': 'Aimee Johnson',\n",
    "    'Andrew': 'Andrew Peters',\n",
    "    'Andrew Peters': 'Andrew Peters',\n",
    "    'Anita': 'Anita Smith',\n",
    "    'Anita Smith': 'Anita Smith',\n",
    "    'Apollo': 'Apollo Blackeagle',\n",
    "    'Apollo Blackeagle': 'Apollo Blackeagle',\n",
    "    'Aquash': 'Mickey Aquash',\n",
    "    'Archie': 'Archie',\n",
    "    'Baxter': 'Eli Baxter',\n",
    "    'Becky': 'Becky',\n",
    "    'Bill': 'Bill Sands',\n",
    "    'Bill Sands': 'Bill Sands',\n",
    "    'Blackbird': 'Jennie Blackbird',\n",
    "    'Blackeagle': 'Apollo Blackeagle',\n",
    "    'Brenda': 'Brenda Wheat',\n",
    "    'Brenda Wheat': 'Brenda Wheat',\n",
    "    'Cal': 'Cal',\n",
    "    'Cameron': 'Cameron',\n",
    "    'Carl': 'Carl Smith',\n",
    "    'Carl Smith (Resource Protection Officer)': 'Carl Smith',\n",
    "    'Carmen': 'Carmen Wrightman',\n",
    "    'Carmen Wrightman': 'Carmen Wrightman',\n",
    "    'Carrie': 'Carrie Isaac',\n",
    "    'Carrie Isaac': 'Carrie Isaac',\n",
    "    'Charles': 'Charles Wright',\n",
    "    'Charles Wright': 'Charles Wright',\n",
    "    'Cheryl': 'Cheryl',\n",
    "    'Chief': 'Chief Gilbert',\n",
    "    'Chief Gilbert': 'Chief Gilbert',\n",
    "    'Chief Joseph Gilbert': 'Chief Gilbert',\n",
    "    'Chris': 'Chris Riley',\n",
    "    'Chris Riley': 'Chris Riley',\n",
    "    'Daniel': 'Daniel',\n",
    "    'Darren': 'Darren',\n",
    "    'Day': '??? Day',\n",
    "    'Dean': 'Dean Jacobs',\n",
    "    'Dean Jacobs': 'Dean Jacobs',\n",
    "    'Dot': 'Dot Peters',\n",
    "    'Dot Peters': 'Dot Peters',\n",
    "    'Doug': 'Doug',\n",
    "    'Doug (Resource Protection Officer)': 'Doug',\n",
    "    'Elaine': 'Elaine Jacobs',\n",
    "    'Elaine Jacobs': 'Elaine Jacobs',\n",
    "    'Eli': 'Eli Baxter',\n",
    "    'Eli Baxter': 'Eli Baxter',\n",
    "    'Eliza': 'Eliza John',\n",
    "    'Eliza John': 'Eliza John',\n",
    "    'Eric': 'Eric Isaac',\n",
    "    'Eric Isaac': 'Eric Isaac',\n",
    "    'Frank': 'Frank',\n",
    "    'Georgina': 'Georgina',\n",
    "    'Gilbert': 'Chief Gilbert',\n",
    "    'Greg': 'Greg Isaac',\n",
    "    'Greg Isaac': 'Greg Isaac',\n",
    "    'Gus': 'Gus',\n",
    "    'Harold': 'Harold Peters',\n",
    "    'Harold Peters': 'Harold Peters',\n",
    "    'Hoeksma': 'Mel Hoeksma',\n",
    "    'Isaac': '??? Isaac',\n",
    "    'Isabelle': 'Isabelle',\n",
    "    'Jacobs': '??? Jacobs',\n",
    "    'Jane': 'Jane Jacobs',\n",
    "    'Jane Jacobs': 'Jane Jacobs',\n",
    "    'Jasper': 'Jasper John',\n",
    "    'Jasper John': 'Jasper John',\n",
    "    'Jean': 'Jean Wrightman',\n",
    "    'Jean Wrightman': 'Jean Wrightman',\n",
    "    'Jen': 'Jennie Blackbird',\n",
    "    'Jennie': 'Jennie Blackbird',\n",
    "    'Jennie Blackbird': 'Jennie Blackbird',\n",
    "    'Jerome': 'Jerome',\n",
    "    'Jerry': 'Jerry',\n",
    "    'Jessica': 'Jessica',\n",
    "    'Joanne': 'Joanne Day',\n",
    "    'Joanne Day': 'Joanne Day',\n",
    "    'Joe': 'Joe Isaac',\n",
    "    'Joe Isaac': 'Joe Isaac',\n",
    "    'John': 'John',\n",
    "    'Johnson': '??? Johnson',\n",
    "    'Jones': '??? Jones',\n",
    "    'Joseph': 'Chief Gilbert',\n",
    "    'Julia': 'Julia',\n",
    "    'Karen': 'Karen Lalleen',\n",
    "    'Karen Lalleen': 'Karen Lalleen',\n",
    "    'Kenneth': 'Kenneth',\n",
    "    'Kennon': 'Kennon Johnson',\n",
    "    'Kennon Johnson': 'Kennon Johnson',\n",
    "    'Kevin': 'Kevin Smith',\n",
    "    'Kevin Smith': 'Kevin Smith',\n",
    "    'Lalleen': 'Karen Lalleen',\n",
    "    'Lee': 'Lee White',\n",
    "    'Lee White': 'Lee White',\n",
    "    'Linda': 'Linda White',\n",
    "    'Linda White': 'Linda White',\n",
    "    'Liz': 'Lizzie Isaac',\n",
    "    'Lizzie': 'Lizzie Isaac',\n",
    "    'Lizzie Isaac': 'Lizzie Isaac',\n",
    "    'Lloyd': 'Lloyd Day',\n",
    "    'Lloyd Day': 'Lloyd Day',\n",
    "    'Lois': 'Lois Wrightman',\n",
    "    'Lois Wrightman': 'Lois Wrightman',\n",
    "    'Lyndsay': 'Lyndsay Sword',\n",
    "    'Lyndsay Sword': 'Lyndsay Sword',\n",
    "    'Mark': 'Mark',\n",
    "    'Mel': 'Mel Hoeksma',\n",
    "    'Mel Hoeksma': 'Mel Hoeksma',\n",
    "    'Mickey': 'Mickey Aquash',\n",
    "    'Mickey Aquash': 'Mickey Aquash',\n",
    "    'Morris': 'Morris Wrightman',\n",
    "    'Morris Wrightman': 'Morris Wrightman',\n",
    "    'Myrna': 'Myrna',\n",
    "    'Naomi': 'Naomi Williams',\n",
    "    'Naomi Williams': 'Naomi Williams',\n",
    "    'PD': 'Puppydog',\n",
    "    'Pat': 'Pat Riley',\n",
    "    'Pat Riley': 'Pat Riley',\n",
    "    'Patricia': 'Patricia',\n",
    "    'Patty': 'Patty Isaac',\n",
    "    'Patty Isaac': 'Patty Isaac',\n",
    "    'Paul': 'Paul',\n",
    "    'Peters': '??? Peters',\n",
    "    'Puppydog': 'Puppydog',\n",
    "    'Rachel': 'Rachel',\n",
    "    'Ralph': 'Ralph ???',\n",
    "    'Ralph Johnson': 'Ralph Johnson',\n",
    "    'Ralph Jones': 'Ralph Jones',\n",
    "    'Riley': '??? Riley',\n",
    "    'Rita': 'Rita Sands',\n",
    "    'Rita Sands': 'Rita Sands',\n",
    "    'Ron': 'Ron',\n",
    "    'Rose': 'Rose',\n",
    "    'Sands': '??? Sands',\n",
    "    'Sarah': 'Sarah',\n",
    "    'Shirley': 'Shirley',\n",
    "    'Smith': '??? Smith',\n",
    "    'Stanley': 'Stanley',\n",
    "    'Stuart': 'Stuart',\n",
    "    'Suzie': 'Suzie ???',\n",
    "    'Suzie Isaac': 'Suzie Isaac',\n",
    "    'Suzie Jones': 'Suzie Jones',\n",
    "    'Sword': 'Lyndsay Sword',\n",
    "    'Terry': 'Terry Sands',\n",
    "    'Terry Sands': 'Terry Sands',\n",
    "    'Tom': 'Tom',\n",
    "    'Vernon': 'Vernon Jones',\n",
    "    'Vernon Jones': 'Vernon Jones',\n",
    "    'Wheat': 'Brenda Wheat',\n",
    "    'White': '??? White',\n",
    "    'Williams': 'Naomi Williams',\n",
    "    'Wright': 'Charles Wright',\n",
    "    'Wrightman': '??? Wrightman',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = 'PROPER_NOUN: {<NNP>+}'\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 60 ms, total: 29.8 s\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g = nx.DiGraph()\n",
    "interviewers_names = ['Dave', 'Rick', 'Clint']\n",
    "for interview in transcripts_df.INTERVIEW:\n",
    "    for index, name, text in interview:\n",
    "        if text:\n",
    "            if name not in interviewers_names:\n",
    "                name = all_names_dict[name]\n",
    "                if not g.has_node(name):\n",
    "                    g.add_node(name)\n",
    "                tagged_text = pos_tag(word_tokenize(text))\n",
    "                parsed_text = cp.parse(tagged_text)\n",
    "                for e in parsed_text:\n",
    "                    if isinstance(e, nltk.tree.Tree) and e.label() == 'PROPER_NOUN':\n",
    "                        names = [word for word, tag in e if len(word) > 1]\n",
    "                        if all_names.intersection(names):\n",
    "                            proper_noun = ' '.join(names)\n",
    "                            if proper_noun in all_names_dict and '???' not in all_names_dict[proper_noun]:\n",
    "                                proper_noun = all_names_dict[proper_noun]\n",
    "                            if proper_noun != name:\n",
    "                                if not g.has_edge(name, proper_noun):\n",
    "                                    g.add_edge(name, proper_noun, weight=0)\n",
    "                                g[name][proper_noun]['weight'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/wordsEn.txt') as f:\n",
    "    english_words = set(word.strip().lower() for word in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 96 ms, total: 29.4 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "h = nx.DiGraph()\n",
    "interviewers_names = ['Dave', 'Rick', 'Clint']\n",
    "for interview in transcripts_df.INTERVIEW:\n",
    "    for index, name, text in interview:\n",
    "        if text:\n",
    "            if name not in interviewers_names:\n",
    "                name = all_names_dict[name]\n",
    "                if not h.has_node(name):\n",
    "                    h.add_node(name, type='interviewee')\n",
    "                tagged_text = pos_tag(word_tokenize(text))\n",
    "                parsed_text = cp.parse(tagged_text)\n",
    "                for e in parsed_text:\n",
    "                    if isinstance(e, nltk.tree.Tree) and e.label() == 'PROPER_NOUN':\n",
    "                        names = [word for word, tag in e if len(word) > 1]\n",
    "                        if names and not all_names.intersection(names) and \\\n",
    "                           (len(names) > 1 or names[0].lower() not in english_words):\n",
    "                            proper_noun = ' '.join(names)\n",
    "                            if proper_noun != name:\n",
    "                                if not h.has_node(proper_noun):\n",
    "                                    h.add_node(proper_noun, type='other')\n",
    "                                if not h.has_edge(name, proper_noun):\n",
    "                                    h.add_edge(name, proper_noun, weight=0)\n",
    "                                h[name][proper_noun]['weight'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(g, 'data/out/people.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(h, 'data/out/other.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
